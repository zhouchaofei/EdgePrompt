"""
MDDæ•°æ®é›†å¤„ç† - å¢å¼ºç‰ˆæœ¬ï¼ˆå¸¦æ ‡å‡†åŒ–ï¼‰
æ·»åŠ äº†ROI z-scoreæ ‡å‡†åŒ–

ä¸»è¦æ”¹è¿›ï¼š
1. åœ¨è®¡ç®—FCå‰å¯¹æ—¶é—´åºåˆ—è¿›è¡Œz-scoreæ ‡å‡†åŒ–
2. æé«˜ä¸åŒè¢«è¯•ä¹‹é—´çš„å¯æ¯”æ€§
"""

import os
import numpy as np
import pandas as pd
import scipy.io as sio
from scipy import stats
import glob
import re
import warnings

warnings.filterwarnings('ignore')


class MDDBaselineProcessor:
    """MDDæ•°æ®å¤„ç†å™¨ - å¢å¼ºç‰ˆæœ¬"""

    def __init__(self, data_folder='./data'):
        """
        Args:
            data_folder: æ•°æ®ä¿å­˜æ ¹ç›®å½•
        """
        self.data_folder = data_folder
        self.mdd_path = os.path.join(data_folder, 'REST-meta-MDD')
        self.baseline_path = os.path.join(self.mdd_path, 'baseline')

        os.makedirs(self.baseline_path, exist_ok=True)

        print(f"=" * 60)
        print(f"MDD Enhanced Baseline Processor")
        print(f"=" * 60)
        print(f"Data path: {self.mdd_path}")
        print(f"Save to: {self.baseline_path}")
        print(f"Features: ROI z-score normalization")
        print(f"=" * 60)

    def z_score_normalize_timeseries(self, timeseries):
        """
        å¯¹æ—¶é—´åºåˆ—è¿›è¡Œz-scoreæ ‡å‡†åŒ–ï¼ˆæŒ‰ROIï¼‰

        Args:
            timeseries: [T, N_ROI] æ—¶é—´åºåˆ—æ•°æ®

        Returns:
            normalized_ts: æ ‡å‡†åŒ–åçš„æ—¶é—´åºåˆ—
        """
        # å¯¹æ¯ä¸ªROIï¼ˆåˆ—ï¼‰è¿›è¡Œz-scoreæ ‡å‡†åŒ–
        # é¿å…é™¤é›¶é”™è¯¯
        epsilon = 1e-8

        mean = np.mean(timeseries, axis=0, keepdims=True)  # [1, N_ROI]
        std = np.std(timeseries, axis=0, keepdims=True) + epsilon  # [1, N_ROI]

        normalized_ts = (timeseries - mean) / std

        # å¤„ç†å¯èƒ½çš„NaNï¼ˆå¦‚æœæŸä¸ªROIå®Œå…¨æ’å®šï¼‰
        normalized_ts = np.nan_to_num(normalized_ts, nan=0.0, posinf=0.0, neginf=0.0)

        return normalized_ts

    def load_roi_signals(self, apply_zscore=True):
        """
        åŠ è½½ROIä¿¡å·æ•°æ®ï¼ˆåªæå–AAL-116ï¼Œå¸¦z-scoreæ ‡å‡†åŒ–ï¼‰

        Args:
            apply_zscore: æ˜¯å¦åº”ç”¨z-scoreæ ‡å‡†åŒ–

        Returns:
            timeseries_list: æ—¶é—´åºåˆ—åˆ—è¡¨ [N_subjects, (T, 116)]
            labels: æ ‡ç­¾åˆ—è¡¨ (0=HC, 1=MDD)
            subject_ids: è¢«è¯•IDåˆ—è¡¨
            site_ids: ç«™ç‚¹IDåˆ—è¡¨ï¼ˆMDDæ•°æ®é›†å¯èƒ½æ˜¯å•ç«™ç‚¹ï¼‰
        """
        print(f"\n1. Loading ROI signals from REST-meta-MDD...")

        if apply_zscore:
            print(f"   âœ“ Z-score normalization will be applied")

        # ROIæ•°æ®è·¯å¾„
        roi_folder = os.path.join(
            self.mdd_path,
            'Results',
            'ROISignals_FunImgARCWF'
        )

        if not os.path.exists(roi_folder):
            raise FileNotFoundError(
                f"ROI folder not found: {roi_folder}\n"
                f"Please ensure REST-meta-MDD data is downloaded and extracted correctly."
            )

        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰.matæ–‡ä»¶
        mat_files = glob.glob(
            os.path.join(roi_folder, '**', '*.mat'),
            recursive=True
        )

        print(f"   Found {len(mat_files)} .mat files")

        if len(mat_files) == 0:
            raise ValueError(f"No .mat files found in {roi_folder}")

        timeseries_list = []
        labels = []
        subject_ids = []
        site_ids = []  # å¯èƒ½éœ€è¦ä»æ–‡ä»¶è·¯å¾„æ¨æ–­

        # ç»Ÿè®¡ä¿¡æ¯
        label_counts = {'MDD': 0, 'HC': 0, 'unknown': 0, 'error': 0}

        # å°è¯•ä»è·¯å¾„æ¨æ–­ç«™ç‚¹ä¿¡æ¯
        site_pattern = re.compile(r'S(\d+)-')  # S1, S2ç­‰å¯èƒ½è¡¨ç¤ºä¸åŒç«™ç‚¹

        for file_idx, file_path in enumerate(mat_files):
            try:
                filename = os.path.basename(file_path)

                # æå–å¯èƒ½çš„ç«™ç‚¹ä¿¡æ¯
                site_match = site_pattern.search(filename)
                if site_match:
                    site_id = f"Site_{site_match.group(1)}"
                else:
                    site_id = 'Site_1'  # é»˜è®¤ç«™ç‚¹

                # ============================================
                # ä»æ–‡ä»¶åæå–æ ‡ç­¾
                # æ ¼å¼: ROISignals_S1-1-0001.mat
                # -1- = MDDæ‚£è€…, -2- = å¥åº·å¯¹ç…§
                # ============================================
                match = re.search(r'-(\d)-', filename)

                if not match:
                    label_counts['unknown'] += 1
                    continue

                group_code = int(match.group(1))

                if group_code == 1:
                    label = 1  # MDD
                    label_counts['MDD'] += 1
                elif group_code == 2:
                    label = 0  # HC
                    label_counts['HC'] += 1
                else:
                    label_counts['unknown'] += 1
                    continue

                # åŠ è½½.matæ–‡ä»¶
                mat_data = sio.loadmat(file_path)

                # æŸ¥æ‰¾æ—¶é—´åºåˆ—æ•°æ®
                if 'ROISignals' not in mat_data:
                    label_counts['error'] += 1
                    continue

                time_series = mat_data['ROISignals']  # [T, 1833]

                # ============================================
                # å…³é”®ï¼šåªæå–AALå›¾è°±çš„116ä¸ªROIï¼ˆåˆ—1-116ï¼‰
                # ============================================
                if time_series.shape[1] < 116:
                    label_counts['error'] += 1
                    continue

                time_series = time_series[:, :116]  # [T, 116]

                # åŸºæœ¬è´¨é‡æ£€æŸ¥
                if time_series.shape[0] < 50:  # æ—¶é—´ç‚¹å¤ªå°‘
                    label_counts['error'] += 1
                    continue

                # ======================
                # å…³é”®æ­¥éª¤ï¼šZ-scoreæ ‡å‡†åŒ–
                # ======================
                if apply_zscore:
                    time_series = self.z_score_normalize_timeseries(time_series)
                else:
                    # å³ä½¿ä¸åšz-scoreï¼Œä¹Ÿè¦æ¸…ç†NaN/Inf
                    time_series = np.nan_to_num(
                        time_series,
                        nan=0.0,
                        posinf=0.0,
                        neginf=0.0
                    )

                # æå–è¢«è¯•ID
                subject_id = filename.split('.')[0]

                # ä¿å­˜æ•°æ®
                timeseries_list.append(time_series)
                labels.append(label)
                subject_ids.append(subject_id)
                site_ids.append(site_id)

                if (file_idx + 1) % 50 == 0:
                    print(f"   Processed: {file_idx + 1}/{len(mat_files)}")

            except Exception as e:
                label_counts['error'] += 1
                if label_counts['error'] <= 5:
                    print(f"   Warning: Failed to process {filename}: {e}")
                continue

        labels = np.array(labels)
        site_ids = np.array(site_ids)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\n   Loading completed:")
        print(f"   ----------------------------------------")
        print(f"   Successfully loaded: {len(timeseries_list)} subjects")
        print(f"     - MDD patients (label=1): {label_counts['MDD']}")
        print(f"     - Healthy controls (label=0): {label_counts['HC']}")
        print(f"   Failed:")
        print(f"     - Unknown label format: {label_counts['unknown']}")
        print(f"     - Processing errors: {label_counts['error']}")

        # éªŒè¯æ ‡ç­¾å¹³è¡¡æ€§
        if label_counts['MDD'] > 0 and label_counts['HC'] > 0:
            ratio = max(label_counts['MDD'], label_counts['HC']) / \
                    min(label_counts['MDD'], label_counts['HC'])
            print(f"\n   Class balance ratio: {ratio:.2f}:1", end=" ")
            if ratio > 3:
                print("(âš ï¸  Imbalanced)")
            else:
                print("(âœ… Balanced)")

        # æ‰“å°æ—¶é—´åºåˆ—é•¿åº¦ä¿¡æ¯
        ts_lengths = [ts.shape[0] for ts in timeseries_list]
        print(f"\n   Time series lengths:")
        print(f"     Min: {min(ts_lengths)}")
        print(f"     Max: {max(ts_lengths)}")
        print(f"     Mean: {np.mean(ts_lengths):.1f}")
        print(f"     Median: {np.median(ts_lengths):.1f}")

        # æ‰“å°ç«™ç‚¹ä¿¡æ¯
        unique_sites = np.unique(site_ids)
        print(f"\n   Sites detected: {len(unique_sites)}")
        for site in unique_sites:
            site_mask = site_ids == site
            site_labels = labels[site_mask]
            n_hc = np.sum(site_labels == 0)
            n_mdd = np.sum(site_labels == 1)
            print(f"     {site}: Total={len(site_labels)} (HC={n_hc}, MDD={n_mdd})")

        return timeseries_list, labels, subject_ids, site_ids

    def compute_fc_matrices(self, timeseries_list, method='pearson'):
        """
        è®¡ç®—åŠŸèƒ½è¿æ¥çŸ©é˜µï¼ˆè¾“å…¥å·²ç»æ˜¯æ ‡å‡†åŒ–åçš„æ—¶é—´åºåˆ—ï¼‰

        Args:
            timeseries_list: æ—¶é—´åºåˆ—åˆ—è¡¨ï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
            method: 'pearson' or 'spearman'

        Returns:
            fc_matrices: FCçŸ©é˜µåˆ—è¡¨ [N_subjects, (116, 116)]
        """
        print(f"\n2. Computing FC matrices ({method} correlation)...")
        print(f"   Note: Using z-score normalized time series")

        fc_matrices = []

        for i, ts in enumerate(timeseries_list):
            # è®¡ç®—ç›¸å…³çŸ©é˜µ
            if method == 'pearson':
                fc = np.corrcoef(ts.T)  # [T, 116] -> [116, 116]
            elif method == 'spearman':
                fc = stats.spearmanr(ts)[0]
            else:
                raise ValueError(f"Unknown method: {method}")

            # å¤„ç†NaNå’ŒInf
            fc = np.nan_to_num(fc, nan=0.0, posinf=1.0, neginf=-1.0)

            # å¯¹è§’çº¿è®¾ä¸º0ï¼ˆå»æ‰è‡ªç›¸å…³ï¼‰
            np.fill_diagonal(fc, 0)

            fc_matrices.append(fc)

            if (i + 1) % 100 == 0:
                print(f"   Computed: {i + 1}/{len(timeseries_list)}")

        fc_matrices = np.array(fc_matrices)

        print(f"\n   FC matrices statistics:")
        print(f"     Shape: {fc_matrices.shape}")
        print(f"     Mean: {fc_matrices.mean():.4f}")
        print(f"     Std: {fc_matrices.std():.4f}")
        print(f"     Min: {fc_matrices.min():.4f}")
        print(f"     Max: {fc_matrices.max():.4f}")

        # æ£€æŸ¥å¼‚å¸¸å€¼
        extreme_values = np.sum(np.abs(fc_matrices) > 0.99)
        total_values = fc_matrices.size
        print(f"     Extreme values (>0.99): {extreme_values} "
              f"({extreme_values / total_values * 100:.2f}%)")

        return fc_matrices

    def save_data(self, fc_matrices, labels, subject_ids, site_ids, method='pearson'):
        """
        ä¿å­˜æ•°æ®ä¸ºnpzæ ¼å¼ï¼ˆåŒ…å«ç«™ç‚¹ä¿¡æ¯ï¼‰

        Args:
            fc_matrices: [N_subjects, 116, 116]
            labels: [N_subjects]
            subject_ids: [N_subjects]
            site_ids: [N_subjects] ç«™ç‚¹ID
            method: FCè®¡ç®—æ–¹æ³•
        """
        print(f"\n3. Saving data...")

        # æ„å»ºæ–‡ä»¶å
        filename = f'mdd_aal116_{method}_fc_normalized.npz'
        save_path = os.path.join(self.baseline_path, filename)

        # ä¿å­˜
        np.savez_compressed(
            save_path,
            fc_matrices=fc_matrices,
            labels=labels,
            subject_ids=subject_ids,
            site_ids=site_ids,  # ä¿å­˜ç«™ç‚¹ä¿¡æ¯
            atlas='aal',
            n_rois=116,
            method=method,
            n_subjects=len(labels),
            normalized=True  # æ ‡è®°å·²åšæ ‡å‡†åŒ–
        )

        print(f"   âœ… Saved to: {save_path}")

        # ä¿å­˜å…ƒä¿¡æ¯ä¸ºæ–‡æœ¬
        meta_file = os.path.join(self.baseline_path, 'mdd_aal116_meta_normalized.txt')
        with open(meta_file, 'w') as f:
            f.write(f"REST-meta-MDD Dataset - Enhanced Baseline Format\n")
            f.write(f"=" * 60 + "\n")
            f.write(f"Atlas: AAL-116\n")
            f.write(f"FC Method: {method}\n")
            f.write(f"Z-score Normalized: Yes\n")
            f.write(f"Number of subjects: {len(labels)}\n")
            f.write(f"Number of ROIs: 116\n")
            f.write(f"FC matrix shape: {fc_matrices.shape}\n")
            f.write(f"\nLabel distribution:\n")
            unique, counts = np.unique(labels, return_counts=True)
            for u, c in zip(unique, counts):
                label_name = 'HC' if u == 0 else 'MDD'
                f.write(f"  {label_name} (label={u}): {c}\n")
            f.write(f"\nFC statistics:\n")
            f.write(f"  Mean: {fc_matrices.mean():.4f}\n")
            f.write(f"  Std: {fc_matrices.std():.4f}\n")
            f.write(f"  Min: {fc_matrices.min():.4f}\n")
            f.write(f"  Max: {fc_matrices.max():.4f}\n")

            # ç«™ç‚¹ä¿¡æ¯
            unique_sites = np.unique(site_ids)
            f.write(f"\nNumber of sites: {len(unique_sites)}\n")

        print(f"   âœ… Meta info saved to: {meta_file}")

        return save_path

    def process_and_save(self, fc_method='pearson', apply_zscore=True):
        """
        å®Œæ•´çš„å¤„ç†æµç¨‹ï¼ˆå¸¦æ ‡å‡†åŒ–ï¼‰

        Args:
            fc_method: FCè®¡ç®—æ–¹æ³•
            apply_zscore: æ˜¯å¦åº”ç”¨z-scoreæ ‡å‡†åŒ–

        Returns:
            save_path: ä¿å­˜è·¯å¾„
        """
        # åŠ è½½æ—¶é—´åºåˆ—ï¼ˆå¸¦æ ‡å‡†åŒ–ï¼‰
        timeseries_list, labels, subject_ids, site_ids = self.load_roi_signals(apply_zscore)

        if len(timeseries_list) == 0:
            print("\nâŒ No valid subjects found!")
            return None

        # è®¡ç®—FCçŸ©é˜µ
        fc_matrices = self.compute_fc_matrices(timeseries_list, fc_method)

        # ä¿å­˜ï¼ˆåŒ…å«ç«™ç‚¹ä¿¡æ¯ï¼‰
        save_path = self.save_data(fc_matrices, labels, subject_ids, site_ids, fc_method)

        print(f"\n{'=' * 60}")
        print(f"âœ… Processing completed successfully!")
        print(f"{'=' * 60}")

        return save_path


def load_mdd_baseline(data_folder='./data', method='pearson', normalized=True):
    """
    åŠ è½½å¤„ç†å¥½çš„MDD baselineæ•°æ®ï¼ˆæ”¯æŒæ ‡å‡†åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        data_folder: æ•°æ®æ ¹ç›®å½•
        method: FCæ–¹æ³•
        normalized: æ˜¯å¦åŠ è½½æ ‡å‡†åŒ–ç‰ˆæœ¬

    Returns:
        fc_matrices: [N, 116, 116]
        labels: [N]
        subject_ids: [N]
        site_ids: [N] ç«™ç‚¹IDï¼ˆå¯èƒ½éƒ½æ˜¯åŒä¸€ç«™ç‚¹ï¼‰
        meta: å…ƒä¿¡æ¯å­—å…¸
    """
    baseline_path = os.path.join(data_folder, 'REST-meta-MDD', 'baseline')

    if normalized:
        filename = f'mdd_aal116_{method}_fc_normalized.npz'
    else:
        filename = f'mdd_aal116_{method}_fc.npz'

    file_path = os.path.join(baseline_path, filename)

    if not os.path.exists(file_path):
        raise FileNotFoundError(
            f"Data file not found: {file_path}\n"
            f"Please run the data preparation script"
        )

    data = np.load(file_path, allow_pickle=True)

    fc_matrices = data['fc_matrices']
    labels = data['labels']
    subject_ids = data['subject_ids']

    # å°è¯•åŠ è½½ç«™ç‚¹ä¿¡æ¯
    site_ids = data['site_ids'] if 'site_ids' in data else None

    meta = {
        'atlas': str(data['atlas']),
        'n_rois': int(data['n_rois']),
        'method': str(data['method']),
        'n_subjects': int(data['n_subjects']),
        'normalized': data.get('normalized', False)
    }

    print(f"Loaded MDD baseline data:")
    print(f"  Shape: {fc_matrices.shape}")
    print(f"  Atlas: {meta['atlas']}")
    print(f"  ROIs: {meta['n_rois']}")
    print(f"  Method: {meta['method']}")
    print(f"  Normalized: {meta['normalized']}")

    if site_ids is not None:
        print(f"  Sites: {len(np.unique(site_ids))}")

    return fc_matrices, labels, subject_ids, site_ids, meta


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description='Prepare REST-meta-MDD data for enhanced baseline experiment'
    )
    parser.add_argument(
        '--data_folder',
        type=str,
        default='./data',
        help='Root folder for data'
    )
    parser.add_argument(
        '--fc_method',
        type=str,
        default='pearson',
        choices=['pearson', 'spearman'],
        help='FC computation method'
    )
    parser.add_argument(
        '--no_zscore',
        action='store_true',
        help='Disable z-score normalization'
    )

    args = parser.parse_args()

    # åˆ›å»ºå¤„ç†å™¨
    processor = MDDBaselineProcessor(data_folder=args.data_folder)

    # å¤„ç†å¹¶ä¿å­˜
    save_path = processor.process_and_save(
        fc_method=args.fc_method,
        apply_zscore=not args.no_zscore  # é»˜è®¤åº”ç”¨z-score
    )

    if save_path:
        print(f"\nğŸ“Š To use this data:")
        print(f"   from mdd_data_baseline import load_mdd_baseline")
        print(f"   fc, labels, ids, site_ids, meta = load_mdd_baseline(normalized=True)")