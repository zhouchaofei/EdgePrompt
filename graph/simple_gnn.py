"""
ç®€å•GNNæ¨¡å‹ç”¨äºéªŒè¯åŠŸèƒ½å›¾å’ŒèŠ‚ç‚¹ç‰¹å¾ç»„åˆ
æ”¯æŒå¤šç§GNNæ¶æ„ï¼šGCN, GAT, GraphSAGE
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv, SAGEConv, global_mean_pool, global_max_pool


class SimpleGNN(nn.Module):
    """ç®€å•çš„GNNåˆ†ç±»å™¨"""

    def __init__(self, input_dim, hidden_dim=64, output_dim=2,
                 num_layers=2, gnn_type='gcn', dropout=0.5,
                 pooling='mean', num_rois=116):  # ğŸ”¥ æ–°å¢ num_rois å‚æ•°
        """
        Args:
            input_dim: èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            output_dim: è¾“å‡ºç±»åˆ«æ•°
            num_layers: GNNå±‚æ•°
            gnn_type: GNNç±»å‹ ('gcn', 'gat', 'sage')
            dropout: dropoutæ¯”ç‡
            pooling: å›¾æ± åŒ–æ–¹å¼ ('mean', 'max', 'mean_max', 'flatten')
            num_rois: ROIæ•°é‡ï¼ˆç”¨äºflatten poolingï¼‰
        """
        super().__init__()

        self.num_layers = num_layers
        self.gnn_type = gnn_type
        self.pooling = pooling
        self.dropout = dropout
        self.num_rois = num_rois  # ğŸ”¥ ä¿å­˜ num_rois

        # GNNå±‚
        self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()

        for i in range(num_layers):
            in_dim = input_dim if i == 0 else hidden_dim
            out_dim = hidden_dim

            if gnn_type == 'gcn':
                self.convs.append(GCNConv(in_dim, out_dim))
            elif gnn_type == 'gat':
                self.convs.append(GATConv(in_dim, out_dim // 4, heads=4, concat=True))
            elif gnn_type == 'sage':
                self.convs.append(SAGEConv(in_dim, out_dim))
            else:
                raise ValueError(f"Unknown gnn_type: {gnn_type}")

            self.batch_norms.append(nn.BatchNorm1d(out_dim))

        # ğŸ”¥ æ ¹æ®poolingæ–¹å¼ç¡®å®šåˆ†ç±»å™¨è¾“å…¥ç»´åº¦
        if pooling == 'flatten':
            classifier_input_dim = hidden_dim * num_rois  # ä¾‹å¦‚: 64 * 116 = 7424
        elif pooling == 'mean_max':
            classifier_input_dim = hidden_dim * 2
        else:  # 'mean' or 'max'
            classifier_input_dim = hidden_dim

        self.classifier = nn.Sequential(
            nn.Linear(classifier_input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, data):
        """
        Args:
            data: PyG Dataå¯¹è±¡

        Returns:
            logits: [batch_size, output_dim]
        """
        x, edge_index, batch = data.x, data.edge_index, data.batch

        # ä½¿ç”¨è¾¹æƒé‡
        if hasattr(data, 'edge_attr') and data.edge_attr is not None:
            edge_weight = data.edge_attr.squeeze()
            if torch.isnan(edge_weight).any() or torch.isinf(edge_weight).any():
                print("âš ï¸ æ£€æµ‹åˆ°NaN/Infè¾¹æƒé‡ï¼Œæ›¿æ¢ä¸º0")
                edge_weight = torch.nan_to_num(edge_weight, nan=0.0, posinf=0.0, neginf=0.0)
        else:
            edge_weight = None

        # GNN layers
        for i in range(self.num_layers):
            if self.gnn_type == 'gat':
                x = self.convs[i](x, edge_index)
            else:
                x = self.convs[i](x, edge_index, edge_weight=edge_weight)

            x = self.batch_norms[i](x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)

        # ğŸ”¥ Graph pooling/readout
        if self.pooling == 'flatten':
            # å°†æ¯ä¸ªå›¾çš„æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾å±•å¹³
            batch_size = int(batch.max().item() + 1)
            # ç¡®ä¿èŠ‚ç‚¹æŒ‰batché¡ºåºæ’åˆ—ï¼ˆPyGæ ‡å‡†è¡Œä¸ºï¼‰
            x = x.view(batch_size, self.num_rois * x.size(1))  # [batch_size, num_rois * hidden_dim]

        elif self.pooling == 'mean':
            x = global_mean_pool(x, batch)
        elif self.pooling == 'max':
            x = global_max_pool(x, batch)
        elif self.pooling == 'mean_max':
            x_mean = global_mean_pool(x, batch)
            x_max = global_max_pool(x, batch)
            x = torch.cat([x_mean, x_max], dim=1)

        # Classification
        logits = self.classifier(x)

        return logits


class LinearProbe(nn.Module):
    """çº¿æ€§æ¢é’ˆæ¨¡å‹ï¼ˆç”¨äºå¿«é€ŸéªŒè¯ç‰¹å¾è´¨é‡ï¼‰"""

    def __init__(self, input_dim, output_dim=2, pooling='mean', num_rois=116):  # ğŸ”¥ æ–°å¢ num_rois
        """
        Args:
            input_dim: èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
            output_dim: è¾“å‡ºç±»åˆ«æ•°
            pooling: å›¾æ± åŒ–æ–¹å¼
            num_rois: ROIæ•°é‡ï¼ˆç”¨äºflatten poolingï¼‰
        """
        super().__init__()

        self.pooling = pooling
        self.num_rois = num_rois  # ğŸ”¥ ä¿å­˜ num_rois

        # ğŸ”¥ æ ¹æ®poolingæ–¹å¼ç¡®å®šåˆ†ç±»å™¨è¾“å…¥ç»´åº¦
        if pooling == 'flatten':
            classifier_input_dim = input_dim * num_rois
        elif pooling == 'mean_max':
            classifier_input_dim = input_dim * 2
        else:
            classifier_input_dim = input_dim

        self.classifier = nn.Linear(classifier_input_dim, output_dim)

    def forward(self, data):
        """
        Args:
            data: PyG Dataå¯¹è±¡

        Returns:
            logits: [batch_size, output_dim]
        """
        x, batch = data.x, data.batch

        # ğŸ”¥ Graph pooling
        if self.pooling == 'flatten':
            batch_size = int(batch.max().item() + 1)
            x = x.view(batch_size, self.num_rois * x.size(1))

        elif self.pooling == 'mean':
            x = global_mean_pool(x, batch)
        elif self.pooling == 'max':
            x = global_max_pool(x, batch)
        elif self.pooling == 'mean_max':
            x_mean = global_mean_pool(x, batch)
            x_max = global_max_pool(x, batch)
            x = torch.cat([x_mean, x_max], dim=1)

        # Classification
        logits = self.classifier(x)

        return logits


class MLPProbe(nn.Module):
    """MLPæ¢é’ˆæ¨¡å‹ï¼ˆç¨å¤æ‚çš„baselineï¼‰"""

    def __init__(self, input_dim, hidden_dim=128, output_dim=2,
                 dropout=0.5, pooling='mean', num_rois=116):  # ğŸ”¥ æ–°å¢ num_rois
        """
        Args:
            input_dim: èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            output_dim: è¾“å‡ºç±»åˆ«æ•°
            dropout: dropoutæ¯”ç‡
            pooling: å›¾æ± åŒ–æ–¹å¼
            num_rois: ROIæ•°é‡ï¼ˆç”¨äºflatten poolingï¼‰
        """
        super().__init__()

        self.pooling = pooling
        self.num_rois = num_rois  # ğŸ”¥ ä¿å­˜ num_rois

        # ğŸ”¥ æ ¹æ®poolingæ–¹å¼ç¡®å®šåˆ†ç±»å™¨è¾“å…¥ç»´åº¦
        if pooling == 'flatten':
            classifier_input_dim = input_dim * num_rois
        elif pooling == 'mean_max':
            classifier_input_dim = input_dim * 2
        else:
            classifier_input_dim = input_dim

        self.classifier = nn.Sequential(
            nn.Linear(classifier_input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, output_dim)
        )

    def forward(self, data):
        """
        Args:
            data: PyG Dataå¯¹è±¡

        Returns:
            logits: [batch_size, output_dim]
        """
        x, batch = data.x, data.batch

        # ğŸ”¥ Graph pooling
        if self.pooling == 'flatten':
            batch_size = int(batch.max().item() + 1)
            x = x.view(batch_size, self.num_rois * x.size(1))

        elif self.pooling == 'mean':
            x = global_mean_pool(x, batch)
        elif self.pooling == 'max':
            x = global_max_pool(x, batch)
        elif self.pooling == 'mean_max':
            x_mean = global_mean_pool(x, batch)
            x_max = global_max_pool(x, batch)
            x = torch.cat([x_mean, x_max], dim=1)

        # Classification
        logits = self.classifier(x)

        return logits


def create_model(model_type, input_dim, hidden_dim=64, output_dim=2,
                 num_layers=2, gnn_type='gcn', dropout=0.5, pooling='mean',
                 num_rois=116):  # ğŸ”¥ æ–°å¢ num_rois å‚æ•°
    """
    åˆ›å»ºæ¨¡å‹çš„å·¥å‚å‡½æ•°

    Args:
        model_type: 'linear', 'mlp', 'gnn'
        input_dim: è¾“å…¥ç»´åº¦
        hidden_dim: éšè—å±‚ç»´åº¦
        output_dim: è¾“å‡ºç»´åº¦
        num_layers: GNNå±‚æ•°
        gnn_type: GNNç±»å‹
        dropout: dropoutæ¯”ç‡
        pooling: æ± åŒ–æ–¹å¼ ('mean', 'max', 'mean_max', 'flatten')
        num_rois: ROIæ•°é‡

    Returns:
        model: PyTorchæ¨¡å‹
    """
    if model_type == 'linear':
        model = LinearProbe(
            input_dim=input_dim,
            output_dim=output_dim,
            pooling=pooling,
            num_rois=num_rois  # ğŸ”¥ ä¼ é€’å‚æ•°
        )
    elif model_type == 'mlp':
        model = MLPProbe(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            output_dim=output_dim,
            dropout=dropout,
            pooling=pooling,
            num_rois=num_rois  # ğŸ”¥ ä¼ é€’å‚æ•°
        )
    elif model_type == 'gnn':
        model = SimpleGNN(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            output_dim=output_dim,
            num_layers=num_layers,
            gnn_type=gnn_type,
            dropout=dropout,
            pooling=pooling,
            num_rois=num_rois  # ğŸ”¥ ä¼ é€’å‚æ•°
        )
    else:
        raise ValueError(f"Unknown model_type: {model_type}")

    return model