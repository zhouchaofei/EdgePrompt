"""
ABIDEæ•°æ®é›†å¤„ç† - å¢å¼ºç‰ˆæœ¬ï¼ˆå¸¦æ ‡å‡†åŒ–å’Œç«™ç‚¹æ•ˆåº”æ£€æŸ¥ï¼‰
æ·»åŠ äº†ROI z-scoreæ ‡å‡†åŒ–å’Œç«™ç‚¹æ•ˆåº”å¯è§†åŒ–

ä¸»è¦æ”¹è¿›ï¼š
1. åœ¨è®¡ç®—FCå‰å¯¹æ—¶é—´åºåˆ—è¿›è¡Œz-scoreæ ‡å‡†åŒ–
2. æå–ç«™ç‚¹ä¿¡æ¯
3. æ”¯æŒç«™ç‚¹æ•ˆåº”æ£€æŸ¥
"""

import os
import numpy as np
import pandas as pd
from nilearn.datasets import fetch_abide_pcp
from scipy import stats
import warnings

warnings.filterwarnings('ignore')


class ABIDEBaselineProcessor:
    """ABIDEæ•°æ®å¤„ç†å™¨ - å¢å¼ºç‰ˆæœ¬"""

    def __init__(self, data_folder='./data', pipeline='cpac', atlas='aal'):
        """
        Args:
            data_folder: æ•°æ®ä¿å­˜æ ¹ç›®å½•
            pipeline: é¢„å¤„ç†ç®¡é“ (cpac/ccs/niak/dparsf)
            atlas: è„‘å›¾è°± (aal/ho/cc200/cc400)
        """
        self.data_folder = data_folder
        self.pipeline = pipeline
        self.atlas = atlas

        self.abide_path = os.path.join(data_folder, 'ABIDE')
        self.baseline_path = os.path.join(self.abide_path, 'baseline')

        os.makedirs(self.baseline_path, exist_ok=True)

        print(f"=" * 60)
        print(f"ABIDE Enhanced Baseline Processor")
        print(f"=" * 60)
        print(f"Pipeline: {pipeline}")
        print(f"Atlas: {atlas}")
        print(f"Save to: {self.baseline_path}")
        print(f"Features: ROI z-score normalization + Site info extraction")
        print(f"=" * 60)

    def z_score_normalize_timeseries(self, timeseries):
        """
        å¯¹æ—¶é—´åºåˆ—è¿›è¡Œz-scoreæ ‡å‡†åŒ–ï¼ˆæŒ‰ROIï¼‰

        Args:
            timeseries: [T, N_ROI] æ—¶é—´åºåˆ—æ•°æ®

        Returns:
            normalized_ts: æ ‡å‡†åŒ–åçš„æ—¶é—´åºåˆ—
        """
        # å¯¹æ¯ä¸ªROIï¼ˆåˆ—ï¼‰è¿›è¡Œz-scoreæ ‡å‡†åŒ–
        # é¿å…é™¤é›¶é”™è¯¯
        epsilon = 1e-8

        mean = np.mean(timeseries, axis=0, keepdims=True)  # [1, N_ROI]
        std = np.std(timeseries, axis=0, keepdims=True) + epsilon  # [1, N_ROI]

        normalized_ts = (timeseries - mean) / std

        # å¤„ç†å¯èƒ½çš„NaNï¼ˆå¦‚æœæŸä¸ªROIå®Œå…¨æ’å®šï¼‰
        normalized_ts = np.nan_to_num(normalized_ts, nan=0.0, posinf=0.0, neginf=0.0)

        return normalized_ts

    def download_and_extract(self, n_subjects=None, apply_zscore=True):
        """
        ä¸‹è½½ABIDEæ•°æ®å¹¶æå–æ—¶é—´åºåˆ—ï¼ˆæ·»åŠ z-scoreæ ‡å‡†åŒ–ï¼‰

        Args:
            n_subjects: ä¸‹è½½çš„è¢«è¯•æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            apply_zscore: æ˜¯å¦åº”ç”¨z-scoreæ ‡å‡†åŒ–

        Returns:
            timeseries_list: æ—¶é—´åºåˆ—åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨ (0=HC, 1=ASD)
            subject_ids: è¢«è¯•IDåˆ—è¡¨
            site_ids: ç«™ç‚¹IDåˆ—è¡¨
        """
        print(f"\n1. Downloading ABIDE data...")
        print(f"   This may take a while on first run...")

        if apply_zscore:
            print(f"   âœ“ Z-score normalization will be applied")

        # ç¡®å®šderivativeså‚æ•°
        if self.atlas == 'aal':
            derivatives = 'rois_aal'
            print(f"   Using AAL-116 atlas")
        elif self.atlas == 'ho':
            derivatives = 'rois_ho'
            print(f"   Using Harvard-Oxford atlas")
        elif self.atlas == 'cc200':
            derivatives = 'rois_cc200'
            print(f"   Using CC200 atlas")
        elif self.atlas == 'cc400':
            derivatives = 'rois_cc400'
            print(f"   Using CC400 atlas")
        else:
            derivatives = 'rois_aal'
            print(f"   Unknown atlas, using AAL-116")

        # ä¸‹è½½æ•°æ®
        data = fetch_abide_pcp(
            data_dir=self.abide_path,
            pipeline=self.pipeline,
            band_pass_filtering=True,
            global_signal_regression=False,
            derivatives=[derivatives],
            n_subjects=n_subjects,
            quality_checked=True,  # åªä¸‹è½½è´¨æ£€é€šè¿‡çš„æ•°æ®
            verbose=1
        )

        print(f"\n   Downloaded {len(data.phenotypic)} subjects")

        # æå–ROIæ—¶é—´åºåˆ—
        print(f"\n2. Extracting time series...")

        timeseries_list = []
        labels = []
        subject_ids = []
        site_ids = []  # æ–°å¢ï¼šç«™ç‚¹ä¿¡æ¯

        # è·å–ROIæ•°æ®
        rois_data = getattr(data, derivatives, None)

        if rois_data is None:
            raise ValueError(f"No ROI data found for atlas: {self.atlas}")

        # è·å–æ ‡ç­¾å’Œç«™ç‚¹ä¿¡æ¯
        phenotypic = data.phenotypic
        dx_labels = phenotypic['DX_GROUP'].values

        # æå–ç«™ç‚¹ä¿¡æ¯ï¼ˆABIDEä¸­çš„SITE_IDåˆ—ï¼‰
        sites = phenotypic['SITE_ID'].values if 'SITE_ID' in phenotypic.columns else None

        valid_count = 0
        invalid_count = 0

        # ç»Ÿè®¡ç«™ç‚¹ä¿¡æ¯
        site_stats = {}

        for idx, roi_file in enumerate(rois_data):
            try:
                # åŠ è½½æ—¶é—´åºåˆ—
                if isinstance(roi_file, str) and os.path.exists(roi_file):
                    ts = pd.read_csv(roi_file, sep='\t', header=0).values
                elif isinstance(roi_file, np.ndarray):
                    ts = roi_file
                else:
                    invalid_count += 1
                    continue

                # åŸºæœ¬è´¨é‡æ£€æŸ¥
                if ts.shape[0] < 50:  # æ—¶é—´ç‚¹å¤ªå°‘
                    invalid_count += 1
                    continue

                # ======================
                # å…³é”®æ­¥éª¤ï¼šZ-scoreæ ‡å‡†åŒ–
                # ======================
                if apply_zscore:
                    ts = self.z_score_normalize_timeseries(ts)
                else:
                    # å³ä½¿ä¸åšz-scoreï¼Œä¹Ÿè¦æ¸…ç†NaN/Inf
                    ts = np.nan_to_num(ts, nan=0.0, posinf=0.0, neginf=0.0)

                # æå–æ ‡ç­¾ã€IDå’Œç«™ç‚¹
                label = dx_labels[idx] - 1  # è½¬ä¸º0/1 (0=HC, 1=ASD)
                subject_id = phenotypic.iloc[idx]['SUB_ID']

                # æå–ç«™ç‚¹ID
                if sites is not None:
                    site_id = sites[idx]
                else:
                    site_id = 'unknown'

                # ä¿å­˜
                timeseries_list.append(ts)
                labels.append(label)
                subject_ids.append(subject_id)
                site_ids.append(site_id)

                # ç»Ÿè®¡ç«™ç‚¹ä¿¡æ¯
                if site_id not in site_stats:
                    site_stats[site_id] = {'HC': 0, 'ASD': 0}
                site_stats[site_id]['HC' if label == 0 else 'ASD'] += 1

                valid_count += 1

                if valid_count % 50 == 0:
                    print(f"   Processed: {valid_count}/{len(rois_data)}")

            except Exception as e:
                invalid_count += 1
                if invalid_count <= 5:
                    print(f"   Warning: Failed to process subject {idx}: {e}")
                continue

        labels = np.array(labels)
        site_ids = np.array(site_ids)

        print(f"\n   Successfully loaded: {valid_count} subjects")
        print(f"   Failed: {invalid_count} subjects")

        # æ‰“å°æ ‡ç­¾åˆ†å¸ƒ
        unique, counts = np.unique(labels, return_counts=True)
        print(f"\n   Label distribution:")
        for u, c in zip(unique, counts):
            label_name = 'HC' if u == 0 else 'ASD'
            print(f"     {label_name} (label={u}): {c} subjects")

        # æ‰“å°ç«™ç‚¹åˆ†å¸ƒ
        if sites is not None:
            print(f"\n   Site distribution:")
            unique_sites = np.unique(site_ids)
            print(f"     Number of sites: {len(unique_sites)}")
            for site in sorted(site_stats.keys()):
                info = site_stats[site]
                total = info['HC'] + info['ASD']
                print(f"     {site}: Total={total} (HC={info['HC']}, ASD={info['ASD']})")

        return timeseries_list, labels, subject_ids, site_ids

    def compute_fc_matrices(self, timeseries_list, method='pearson'):
        """
        è®¡ç®—åŠŸèƒ½è¿æ¥çŸ©é˜µï¼ˆè¾“å…¥å·²ç»æ˜¯æ ‡å‡†åŒ–åçš„æ—¶é—´åºåˆ—ï¼‰

        Args:
            timeseries_list: æ—¶é—´åºåˆ—åˆ—è¡¨ï¼ˆå·²æ ‡å‡†åŒ–ï¼‰
            method: 'pearson' or 'spearman'

        Returns:
            fc_matrices: FCçŸ©é˜µåˆ—è¡¨ [N_subjects, (N_ROI, N_ROI)]
        """
        print(f"\n3. Computing FC matrices ({method} correlation)...")
        print(f"   Note: Using z-score normalized time series")

        fc_matrices = []

        for i, ts in enumerate(timeseries_list):
            # è®¡ç®—ç›¸å…³çŸ©é˜µ
            if method == 'pearson':
                fc = np.corrcoef(ts.T)  # [T, N_ROI] -> [N_ROI, N_ROI]
            elif method == 'spearman':
                fc = stats.spearmanr(ts)[0]
            else:
                raise ValueError(f"Unknown method: {method}")

            # å¤„ç†NaNå’ŒInf
            fc = np.nan_to_num(fc, nan=0.0, posinf=1.0, neginf=-1.0)

            # å¯¹è§’çº¿è®¾ä¸º0ï¼ˆå»æ‰è‡ªç›¸å…³ï¼‰
            np.fill_diagonal(fc, 0)

            fc_matrices.append(fc)

            if (i + 1) % 100 == 0:
                print(f"   Computed: {i + 1}/{len(timeseries_list)}")

        fc_matrices = np.array(fc_matrices)

        print(f"   FC matrices shape: {fc_matrices.shape}")
        print(f"   FC statistics:")
        print(f"     Mean: {fc_matrices.mean():.4f}")
        print(f"     Std: {fc_matrices.std():.4f}")
        print(f"     Min: {fc_matrices.min():.4f}")
        print(f"     Max: {fc_matrices.max():.4f}")

        return fc_matrices

    def save_data(self, fc_matrices, labels, subject_ids, site_ids, method='pearson'):
        """
        ä¿å­˜æ•°æ®ä¸ºnpzæ ¼å¼ï¼ˆåŒ…å«ç«™ç‚¹ä¿¡æ¯ï¼‰

        Args:
            fc_matrices: [N_subjects, N_ROI, N_ROI]
            labels: [N_subjects]
            subject_ids: [N_subjects]
            site_ids: [N_subjects] ç«™ç‚¹ID
            method: FCè®¡ç®—æ–¹æ³•
        """
        print(f"\n4. Saving data...")

        # æ„å»ºæ–‡ä»¶å
        filename = f'abide_{self.atlas}_{method}_fc_normalized.npz'
        save_path = os.path.join(self.baseline_path, filename)

        # ä¿å­˜
        np.savez_compressed(
            save_path,
            fc_matrices=fc_matrices,
            labels=labels,
            subject_ids=subject_ids,
            site_ids=site_ids,  # æ–°å¢ï¼šä¿å­˜ç«™ç‚¹ä¿¡æ¯
            atlas=self.atlas,
            method=method,
            n_subjects=len(labels),
            n_rois=fc_matrices.shape[1],
            normalized=True  # æ ‡è®°å·²åšæ ‡å‡†åŒ–
        )

        print(f"   Saved to: {save_path}")

        # ä¿å­˜å…ƒä¿¡æ¯ä¸ºæ–‡æœ¬
        meta_file = os.path.join(self.baseline_path, f'abide_{self.atlas}_meta_normalized.txt')
        with open(meta_file, 'w') as f:
            f.write(f"ABIDE Dataset - Enhanced Baseline Format\n")
            f.write(f"=" * 60 + "\n")
            f.write(f"Atlas: {self.atlas}\n")
            f.write(f"Pipeline: {self.pipeline}\n")
            f.write(f"FC Method: {method}\n")
            f.write(f"Z-score Normalized: Yes\n")
            f.write(f"Number of subjects: {len(labels)}\n")
            f.write(f"Number of ROIs: {fc_matrices.shape[1]}\n")
            f.write(f"FC matrix shape: {fc_matrices.shape}\n")
            f.write(f"\nLabel distribution:\n")
            unique, counts = np.unique(labels, return_counts=True)
            for u, c in zip(unique, counts):
                label_name = 'HC' if u == 0 else 'ASD'
                f.write(f"  {label_name} (label={u}): {c}\n")

            # ç«™ç‚¹ä¿¡æ¯
            if site_ids is not None:
                unique_sites = np.unique(site_ids)
                f.write(f"\nNumber of sites: {len(unique_sites)}\n")

        print(f"   Meta info saved to: {meta_file}")

        return save_path

    def process_and_save(self, n_subjects=None, fc_method='pearson', apply_zscore=True):
        """
        å®Œæ•´çš„å¤„ç†æµç¨‹ï¼ˆå¸¦æ ‡å‡†åŒ–ï¼‰

        Args:
            n_subjects: è¢«è¯•æ•°é‡
            fc_method: FCè®¡ç®—æ–¹æ³•
            apply_zscore: æ˜¯å¦åº”ç”¨z-scoreæ ‡å‡†åŒ–

        Returns:
            save_path: ä¿å­˜è·¯å¾„
        """
        # ä¸‹è½½å’Œæå–æ—¶é—´åºåˆ—ï¼ˆå¸¦æ ‡å‡†åŒ–ï¼‰
        timeseries_list, labels, subject_ids, site_ids = self.download_and_extract(
            n_subjects, apply_zscore
        )

        if len(timeseries_list) == 0:
            print("\nâŒ No valid subjects found!")
            return None

        # è®¡ç®—FCçŸ©é˜µ
        fc_matrices = self.compute_fc_matrices(timeseries_list, fc_method)

        # ä¿å­˜ï¼ˆåŒ…å«ç«™ç‚¹ä¿¡æ¯ï¼‰
        save_path = self.save_data(fc_matrices, labels, subject_ids, site_ids, fc_method)

        print(f"\n{'=' * 60}")
        print(f"âœ… Processing completed successfully!")
        print(f"{'=' * 60}")

        return save_path


def load_abide_baseline(data_folder='./data', atlas='aal', method='pearson', normalized=True):
    """
    åŠ è½½å¤„ç†å¥½çš„ABIDE baselineæ•°æ®ï¼ˆæ”¯æŒæ ‡å‡†åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        data_folder: æ•°æ®æ ¹ç›®å½•
        atlas: è„‘å›¾è°±
        method: FCæ–¹æ³•
        normalized: æ˜¯å¦åŠ è½½æ ‡å‡†åŒ–ç‰ˆæœ¬

    Returns:
        fc_matrices: [N, N_ROI, N_ROI]
        labels: [N]
        subject_ids: [N]
        site_ids: [N] ç«™ç‚¹ID
        meta: å…ƒä¿¡æ¯å­—å…¸
    """
    baseline_path = os.path.join(data_folder, 'ABIDE', 'baseline')

    if normalized:
        filename = f'abide_{atlas}_{method}_fc_normalized.npz'
    else:
        filename = f'abide_{atlas}_{method}_fc.npz'

    file_path = os.path.join(baseline_path, filename)

    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Data file not found: {file_path}\n"
                                f"Please run the data preparation script")

    data = np.load(file_path, allow_pickle=True)

    fc_matrices = data['fc_matrices']
    labels = data['labels']
    subject_ids = data['subject_ids']

    # å°è¯•åŠ è½½ç«™ç‚¹ä¿¡æ¯
    site_ids = data['site_ids'] if 'site_ids' in data else None

    meta = {
        'atlas': str(data['atlas']),
        'method': str(data['method']),
        'n_subjects': int(data['n_subjects']),
        'n_rois': int(data['n_rois']),
        'normalized': data.get('normalized', False)
    }

    print(f"Loaded ABIDE baseline data:")
    print(f"  Shape: {fc_matrices.shape}")
    print(f"  Atlas: {meta['atlas']}")
    print(f"  Method: {meta['method']}")
    print(f"  Normalized: {meta['normalized']}")

    if site_ids is not None:
        print(f"  Sites: {len(np.unique(site_ids))}")

    return fc_matrices, labels, subject_ids, site_ids, meta


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Prepare ABIDE data for enhanced baseline experiment')
    parser.add_argument('--data_folder', type=str, default='./data',
                        help='Root folder for data')
    parser.add_argument('--pipeline', type=str, default='cpac',
                        choices=['cpac', 'ccs', 'niak', 'dparsf'],
                        help='Preprocessing pipeline')
    parser.add_argument('--atlas', type=str, default='aal',
                        choices=['aal', 'ho', 'cc200', 'cc400'],
                        help='Brain atlas')
    parser.add_argument('--n_subjects', type=int, default=None,
                        help='Number of subjects to download (None=all)')
    parser.add_argument('--fc_method', type=str, default='pearson',
                        choices=['pearson', 'spearman'],
                        help='FC computation method')
    parser.add_argument('--no_zscore', action='store_true',
                        help='Disable z-score normalization')

    args = parser.parse_args()

    # åˆ›å»ºå¤„ç†å™¨
    processor = ABIDEBaselineProcessor(
        data_folder=args.data_folder,
        pipeline=args.pipeline,
        atlas=args.atlas
    )

    # å¤„ç†å¹¶ä¿å­˜
    save_path = processor.process_and_save(
        n_subjects=args.n_subjects,
        fc_method=args.fc_method,
        apply_zscore=not args.no_zscore  # é»˜è®¤åº”ç”¨z-score
    )

    if save_path:
        print(f"\nğŸ“Š To use this data:")
        print(f"   from abide_data_baseline import load_abide_baseline")
        print(f"   fc, labels, ids, site_ids, meta = load_abide_baseline(normalized=True)")