"""
å‡†å¤‡GNNå®éªŒæ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰
ä½¿ç”¨é¢„è®­ç»ƒçš„Node Encoderæå–ç‰¹å¾
åŒ…å« load_gnn_dataset å‡½æ•°
"""

import os
import numpy as np
import torch
from torch_geometric.data import Data
import pickle
from datetime import datetime
import argparse

from fc_construction import FCConstructor
from node_features import StatisticalFeatureExtractor  # âœ… åªå¯¼å…¥ç»Ÿè®¡ç‰¹å¾
from extract_node_features import extract_node_features_pretrained  # âœ… å¯¼å…¥é¢„è®­ç»ƒç‰ˆæœ¬


def load_timeseries_data(dataset_name, data_folder='./data'):
    """åŠ è½½æ—¶é—´åºåˆ—æ•°æ®"""
    print(f"\n{'='*80}")
    print(f"Loading {dataset_name} time series data...")
    print(f"{'='*80}")

    if dataset_name == 'ABIDE':
        from abide_data_baseline import ABIDEBaselineProcessor
        processor = ABIDEBaselineProcessor(data_folder=data_folder)
        timeseries_list, labels, subject_ids, site_ids = processor.download_and_extract(
            n_subjects=None, apply_zscore=True
        )

    elif dataset_name == 'MDD':
        from mdd_data_baseline import MDDBaselineProcessor
        processor = MDDBaselineProcessor(data_folder=data_folder)
        timeseries_list, labels, subject_ids, site_ids = processor.load_roi_signals(
            apply_zscore=True
        )

    else:
        raise ValueError(f"Unknown dataset: {dataset_name}")

    print(f"\nâœ“ Loaded {len(labels)} subjects")
    print(f"  ROIs: {timeseries_list[0].shape[1]}")
    print(f"  Label distribution: {np.bincount(labels)}")

    return timeseries_list, labels, subject_ids, site_ids


def construct_functional_graphs(timeseries_list, methods=['pearson', 'ledoit_wolf']):
    """æ„å»ºåŠŸèƒ½è¿æ¥å›¾"""
    print(f"\n{'='*80}")
    print(f"Constructing functional connectivity graphs...")
    print(f"{'='*80}")

    fc_dict = {}

    for method in methods:
        print(f"\nMethod: {method}")
        constructor = FCConstructor(method=method)

        fc_matrices = []
        for i, ts in enumerate(timeseries_list):
            fc = constructor.compute_fc_matrix(ts)
            fc_matrices.append(fc)

            if (i + 1) % 100 == 0:
                print(f"  Processed: {i+1}/{len(timeseries_list)}")

        fc_matrices = np.array(fc_matrices)
        fc_dict[method] = fc_matrices

        print(f"  âœ“ {method}: shape={fc_matrices.shape}")
        print(f"    Stats: mean={fc_matrices.mean():.4f}, "
              f"std={fc_matrices.std():.4f}")

    return fc_dict


def extract_statistical_features(timeseries_list):
    """æå–ç»Ÿè®¡ç‰¹å¾"""
    print(f"\n{'='*80}")
    print("Extracting statistical node features...")
    print(f"{'='*80}")

    extractor = StatisticalFeatureExtractor()
    features_list = []

    for i, ts in enumerate(timeseries_list):
        features = extractor.extract_features(ts)
        features_list.append(features)

        if (i + 1) % 100 == 0:
            print(f"  Processed: {i+1}/{len(timeseries_list)}")

    features_array = np.array(features_list)
    feature_dim = extractor.get_feature_dim()

    print(f"\nâœ“ Statistical features extracted")
    print(f"  Feature dim: {feature_dim}")
    print(f"  Shape: {features_array.shape}")

    return features_array, feature_dim


def extract_pretrained_features(timeseries_list, encoder_path, embedding_dim, device):
    """âœ… ä½¿ç”¨é¢„è®­ç»ƒencoderæå–ç‰¹å¾"""
    print(f"\n{'='*80}")
    print("Extracting pretrained node features...")
    print(f"{'='*80}")

    # âœ… è°ƒç”¨æ­£ç¡®çš„å‡½æ•°
    features_list = extract_node_features_pretrained(
        timeseries_list=timeseries_list,
        encoder_path=encoder_path,
        embedding_dim=embedding_dim,
        device=device
    )

    features_array = np.array(features_list)

    print(f"\nâœ“ Pretrained features extracted")
    print(f"  Feature dim: {embedding_dim}")
    print(f"  Shape: {features_array.shape}")

    return features_array, embedding_dim


def create_pyg_graphs(fc_matrices, node_features, labels, top_k=20):
    """
    åˆ›å»ºPyTorch Geometricå›¾å¯¹è±¡ï¼ˆTop-Kç¨€ç–åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        fc_matrices: FCçŸ©é˜µæ•°ç»„ [N_subjects, N_ROI, N_ROI]
        node_features: èŠ‚ç‚¹ç‰¹å¾æ•°ç»„ [N_subjects, N_ROI, feature_dim]
        labels: æ ‡ç­¾æ•°ç»„
        top_k: æ¯ä¸ªèŠ‚ç‚¹ä¿ç•™æœ€å¼ºçš„kä¸ªè¿æ¥ï¼ˆé»˜è®¤20ï¼Œçº¦å 116èŠ‚ç‚¹çš„17%ï¼‰

    Returns:
        graph_list: PyG Dataå¯¹è±¡åˆ—è¡¨
    """
    print(f"\nåˆ›å»ºPyGå›¾å¯¹è±¡ï¼ˆTop-Kç¨€ç–åŒ–ï¼Œk={top_k}ï¼‰...")

    graph_list = []
    n_subjects = len(fc_matrices)
    invalid_count = 0

    for i in range(n_subjects):
        fc = fc_matrices[i].copy()
        x = node_features[i]
        y = labels[i]

        # ===== 1. æ¸…ç†æ— æ•ˆå€¼ =====
        if np.any(np.isnan(fc)) or np.any(np.isinf(fc)):
            fc = np.nan_to_num(fc, nan=0.0, posinf=0.0, neginf=0.0)
            invalid_count += 1

        if np.any(np.isnan(x)) or np.any(np.isinf(x)):
            x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)
            invalid_count += 1

        # ===== 2. å¤„ç†FCçŸ©é˜µ =====
        # å–ç»å¯¹å€¼ï¼ˆè¿æ¥å¼ºåº¦ï¼‰
        fc_abs = np.abs(fc)

        # å¯¹è§’çº¿è®¾ä¸º0ï¼ˆå»é™¤è‡ªç¯ï¼‰
        np.fill_diagonal(fc_abs, 0)

        # ===== 3. ğŸ”¥ Top-K ç¨€ç–åŒ– =====
        num_nodes = fc_abs.shape[0]
        k = min(top_k, num_nodes - 1)  # é˜²æ­¢kè¶…è¿‡èŠ‚ç‚¹æ•°

        # å¯¹æ¯ä¸€è¡Œï¼Œæ‰¾å‡ºæœ€å¼ºçš„kä¸ªè¿æ¥
        # argsortè¿”å›ä»å°åˆ°å¤§çš„ç´¢å¼•ï¼Œå–æœ€åkä¸ª
        topk_indices = np.argsort(fc_abs, axis=1)[:, -k:]

        # æ„å»ºç¨€ç–è¾¹åˆ—è¡¨
        edge_index_list = []
        edge_attr_list = []

        for row in range(num_nodes):
            for col in topk_indices[row]:
                if fc_abs[row, col] > 0:  # é¢å¤–ä¿é™©
                    edge_index_list.append([row, col])
                    edge_attr_list.append(fc_abs[row, col])

        # è½¬æ¢ä¸ºTensor
        if len(edge_index_list) == 0:
            # å¦‚æœæ²¡æœ‰è¾¹ï¼Œåˆ›å»ºä¸€ä¸ªæœ€å°å›¾ï¼ˆæ¯ä¸ªèŠ‚ç‚¹è¿æ¥åˆ°è‡ªå·±ï¼‰
            edge_index = torch.arange(num_nodes).repeat(2, 1)
            edge_attr = torch.ones(num_nodes, 1) * 0.01
        else:
            edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()
            edge_attr = torch.tensor(edge_attr_list, dtype=torch.float).unsqueeze(1)

        # ===== 4. åˆ›å»ºPyG Dataå¯¹è±¡ =====
        data = Data(
            x=torch.FloatTensor(x),
            edge_index=edge_index,
            edge_attr=edge_attr,
            y=torch.LongTensor([y])
        )

        # æœ€ç»ˆéªŒè¯
        assert not torch.isnan(data.x).any(), f"è¢«è¯• {i} çš„xä»åŒ…å«NaN"
        assert not torch.isnan(data.edge_attr).any(), f"è¢«è¯• {i} çš„edge_atträ»åŒ…å«NaN"

        graph_list.append(data)

        if (i + 1) % 100 == 0:
            print(f"  åˆ›å»ºè¿›åº¦: {i + 1}/{n_subjects}")

    if invalid_count > 0:
        print(f"  âš ï¸  æ¸…ç†äº† {invalid_count} ä¸ªè¢«è¯•çš„æ— æ•ˆå€¼")

    # ç»Ÿè®¡ä¿¡æ¯
    avg_edges = np.mean([g.edge_index.shape[1] for g in graph_list])
    num_nodes = graph_list[0].x.shape[0]
    sparsity = avg_edges / (num_nodes * (num_nodes - 1)) * 100

    print(f"  âœ“ åˆ›å»ºäº† {len(graph_list)} ä¸ªå›¾")
    print(f"    èŠ‚ç‚¹æ•°: {num_nodes}")
    print(f"    èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {graph_list[0].x.shape[1]}")
    print(f"    å¹³å‡è¾¹æ•°: {avg_edges:.1f}")
    print(f"    ç¨€ç–åº¦: {sparsity:.2f}% (Top-K={top_k})")

    return graph_list


def save_gnn_dataset(save_dir, dataset_name, fc_dict, features_dict,
                     labels, subject_ids, site_ids, top_k=20):
    """ä¿å­˜GNNæ•°æ®é›†ï¼ˆæ·»åŠ top_kå‚æ•°ï¼‰"""
    os.makedirs(save_dir, exist_ok=True)

    print(f"\n{'='*80}")
    print(f"ä¿å­˜GNNæ•°æ®é›†ï¼ˆTop-K={top_k}ï¼‰...")
    print(f"{'='*80}")

    saved_files = []

    for fc_method, fc_matrices in fc_dict.items():
        for feature_type, node_features in features_dict.items():
            print(f"\nå¤„ç†: {fc_method} + {feature_type}")

            # ğŸ”¥ ä½¿ç”¨ Top-K ç¨€ç–åŒ–
            graph_list = create_pyg_graphs(
                fc_matrices=fc_matrices,
                node_features=node_features,
                labels=labels,
                top_k=top_k  # ä¼ å…¥top_kå‚æ•°
            )

            # ä¿å­˜
            filename = f"{dataset_name}_{fc_method}_{feature_type}.pkl"
            filepath = os.path.join(save_dir, filename)

            data_dict = {
                'graph_list': graph_list,
                'labels': labels,
                'subject_ids': subject_ids,
                'site_ids': site_ids,
                'metadata': {
                    'dataset': dataset_name,
                    'fc_method': fc_method,
                    'feature_type': feature_type,
                    'n_subjects': len(labels),
                    'n_nodes': graph_list[0].x.shape[0],
                    'node_feature_dim': graph_list[0].x.shape[1],
                    'top_k': top_k,  # è®°å½•ç¨€ç–åŒ–å‚æ•°
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
            }

            with open(filepath, 'wb') as f:
                pickle.dump(data_dict, f)

            print(f"  âœ“ ä¿å­˜è‡³: {filepath}")
            saved_files.append(filepath)

    return saved_files


def load_gnn_dataset(filepath):
    """
    åŠ è½½å·²ä¿å­˜çš„GNNæ•°æ®é›†

    Args:
        filepath: .pklæ–‡ä»¶è·¯å¾„

    Returns:
        graph_list: List[Data]
        labels: np.array
        metadata: dict
    """
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Dataset file not found: {filepath}")

    print(f"Loading dataset from: {filepath}")

    with open(filepath, 'rb') as f:
        data_dict = pickle.load(f)

    graph_list = data_dict['graph_list']
    labels = data_dict['labels']
    metadata = data_dict.get('metadata', {})

    return graph_list, labels, metadata


def main():
    parser = argparse.ArgumentParser(description='Prepare GNN data with Top-K sparsification')
    parser.add_argument('--dataset', type=str, required=True,
                        choices=['ABIDE', 'MDD'])
    parser.add_argument('--data_folder', type=str, default='./data')
    parser.add_argument('--save_dir', type=str, default='./data/gnn_datasets')
    parser.add_argument('--encoder_path', type=str,
                        default='./pretrained_models/node_encoder_best.pth',
                        help='Path to pretrained encoder')
    parser.add_argument('--embedding_dim', type=int, default=64)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--top_k', type=int, default=20,  # ğŸ”¥ æ–°å¢å‚æ•°
                        help='Keep top-k strongest connections per node')

    args = parser.parse_args()

    print(f"\n{'=' * 80}")
    print(f"GNNæ•°æ®å‡†å¤‡ï¼ˆTop-Kç¨€ç–åŒ–ï¼‰")
    print(f"{'=' * 80}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"Top-K: {args.top_k} (ä¿ç•™æ¯ä¸ªèŠ‚ç‚¹æœ€å¼ºçš„{args.top_k}ä¸ªè¿æ¥)")
    print(f"Encoder: {args.encoder_path}")
    print(f"{'=' * 80}\n")

    # 1-3. æ•°æ®åŠ è½½å’Œç‰¹å¾æå–ï¼ˆä¿æŒä¸å˜ï¼‰
    timeseries_list, labels, subject_ids, site_ids = load_timeseries_data(
        args.dataset, args.data_folder
    )

    fc_dict = construct_functional_graphs(
        timeseries_list,
        methods=['pearson', 'ledoit_wolf']
    )

    features_dict = {}

    # åªä½¿ç”¨é¢„è®­ç»ƒç‰¹å¾ï¼ˆæ ¹æ®ä½ çš„æ³¨é‡Šï¼Œstatisticalå·²è¢«è¯æ˜æ— æ•ˆï¼‰
    if os.path.exists(args.encoder_path):
        pretrained_features, pretrained_dim = extract_pretrained_features(
            timeseries_list=timeseries_list,
            encoder_path=args.encoder_path,
            embedding_dim=args.embedding_dim,
            device=args.device
        )
        features_dict['temporal'] = pretrained_features
    else:
        print(f"\nâš ï¸  é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {args.encoder_path}")
        print("  è·³è¿‡é¢„è®­ç»ƒç‰¹å¾æå–")

    # 4. ä¿å­˜æ•°æ®ï¼ˆä¼ å…¥top_kï¼‰
    saved_files = save_gnn_dataset(
        save_dir=args.save_dir,
        dataset_name=args.dataset,
        fc_dict=fc_dict,
        features_dict=features_dict,
        labels=labels,
        subject_ids=subject_ids,
        site_ids=site_ids,
        top_k=args.top_k  # ğŸ”¥ ä¼ å…¥top_kå‚æ•°
    )

    print(f"\n{'=' * 80}")
    print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
    print(f"{'=' * 80}")
    print(f"\nç”Ÿæˆäº† {len(saved_files)} ä¸ªæ•°æ®é›†æ–‡ä»¶")
    print(f"ç¨€ç–åŒ–ç­–ç•¥: Top-K={args.top_k}")


if __name__ == '__main__':
    main()